{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPSaU-uRo0Dm"
   },
   "source": [
    "# **TAREA 1a: Graph Neural Networks (Teoría y ejemplos)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcwprQJBo0Dr"
   },
   "source": [
    "En esta tarea trataremos la aplicación de redes neuronales en grafos. Las Graph Neural Networks (GNNs) han crecido en popularidad tanto en aplicaciones como en investigación, incluyendo campos como redes sociales, knowledge graphs, sistemas de recomendación y bioinformática. Aunque la teoría y las matemáticas que subyacen a las GNN pueden parecer complicadas en un principio, la implementación de estos modelos es bastante sencilla y ayuda a comprender la metodología. Por lo tanto, analizaremos la implementación de las capas básicas de una GNN, que son las graph convolutions y las attention layers. Por último, aplicaremos una GNN a tareas a nivel de nodo, de arista y de grafo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47lWcbP8o0Dr",
    "outputId": "84c5645d-b0dc-4cbd-a313-fb052e95ca98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/bg12rs4d7n3c6p64vwtqhqqh0000gn/T/ipykernel_7443/3717313254.py:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # Para exportarlo\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Librerias\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "## Imports gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # Para exportarlo\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Barra de progreso\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10 #dataset de imágenes de 10 clases diferentes\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path donde cargar/guardar los datasets\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path donde guardar los modelos pre-entrenados\n",
    "CHECKPOINT_PATH = \"../saved_models/tarea1\"\n",
    "\n",
    "# Semilla\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Poner las operaciones deterministicas en la GPU para poder reproducirlas\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvRpp9u2o0Du"
   },
   "source": [
    "Antes de seguir, se deben incluir en la carpeta \"tarea1\" los tres ficheros siguientes: \"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\" y \"GraphLevelGraphConv.ckpt\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-7NLdD4o0Dv"
   },
   "source": [
    "## Un poquito de teoría de Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpjjFeYko0Dv"
   },
   "source": [
    "### ¿Cómo representar un grafo?\n",
    "\n",
    "Antes de nada, vamos a definir como se representa un grafo. Matemáticamente, un grafo $\\mathcal{G}$ se define como una tupla de un conjunto de nodos $V$, y un conjunto de aristas $E$: $\\mathcal{G}=(V,E)$. Cada arista se representa como un par de valores (nodos), y representan una conexión entre ellos.\n",
    "Los aristas pueden tener pesos o no (weighted o unweighted) y tener dirección o no (directed o undirected).\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/example_graph.svg\" width=\"250px\"></center>\n",
    "\n",
    "Los nodos son $V=\\{1,2,3,4\\}$, y las aristas $E=\\{(1,2), (2,3), (3,4), (2,4)\\}$. Por simplicidad, es unweighted y undirected, y deberían incluirse los pares simétricos (por ejemplo, el $(2,1)$). Normalmente, para las aristas, se usa para representar los valores y operar con ellas la matriz de adyacencia o la lista de pares de nodos .\n",
    "\n",
    "La **Matriz de Adyacencia** $A$ es una matriz cuadrada cuyos elementos indican todos los pares denodos que son adyacentes, es decir, conectados o no. En el caso más simple, $A_{ij}$ = 1 si hay conexión entre los nodos $i$ y $j$, y 0 en el caso de que no haya conexión. Si los aristas tuviesen peso, ese valor sustituiría a los unos. Un grafo no dirigido es simétrico, y por lo tanto ($A_{ij}=A_{ji}$). Para el ejemplo, tenemos la matriz de adyacencia siguiente:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    0 & 1 & 0 & 0\\\\\n",
    "    1 & 0 & 1 & 1\\\\\n",
    "    0 & 1 & 0 & 1\\\\\n",
    "    0 & 1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "En esta tarea usaremos la matriz de adyacencia para que se vea más simple y claro visualmente, pero en terminos de eficiencia de memoria es mejor las listas de aristas (y las librerías suelen implementarlas más).\n",
    "Se puede pasar de listas a matrices dispersas (sparse), haciendolo eficiente, mediante el subpaquete`torch.sparse` ([documentación](https://pytorch.org/docs/stable/sparse.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb2ezWRGo0Dv"
   },
   "source": [
    "### Convoluciones de grafos (Graph Convolutions, GCNs)\n",
    "\n",
    "Las GCNs, introducidas por [Kipf et al.](https://openreview.net/pdf?id=SJU4ayYgl) in 2016 [blog](https://tkipf.github.io/graph-convolutional-networks/), son parecidas a las convoluciones en imágenes en el sentido de filtrado de parámetros compartido en diferentes localizaciones del grafo. Al mismo tiempo, se basan en métodos de paso de mensajes, lo que significa que los nodos intercambian información con los vecinos y se envían \"mensajes\" entre sí. Antes de ver la notación matemática, podemos intentar comprender visualmente cómo funcionan las GCN.\n",
    "En un primer paso, cada nodo crea un vector de características que representa el mensaje que quiere enviar a todos sus vecinos. En el segundo paso, los mensajes se envían a los vecinos, de forma que un nodo recibe un mensaje por nodo adyacente o vecino. A continuación se muestran ambos pasos con el ejemplo anterior :\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://miro.medium.com/v2/resize:fit:1246/0*zhmuVMqj8pgvJI3O\" width=\"700px\"></center>\n",
    "\n",
    "Si queremos formularlo en términos más matemáticos, primero tenemos que decidir cómo combinar todos los mensajes que recibe un nodo. Como el número de mensajes varía de un nodo a otro, necesitamos una operación que funcione para cualquier número. Por eso, lo habitual es la suma o la media. Dadas las características anteriores de los nodos $H^{(l)}$, la capa GCN se define como:\n",
    "\n",
    "$$H^{(l+1)} = \\sigma\\left(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)}\\right)$$\n",
    "\n",
    "$W^{(l)}$ son los pesos de los parámetros con los que transformamos las características de entrada en mensajes ($H^{(l)}W^{(l)}$). A la matriz de adyacencia $A$ le añadimos la matriz identidad para que cada nodo se envíe su propio mensaje también a sí mismo, representándolo así: $\\hat{A}=A+I$. Por último, para sacar la media en este caso, calculamos la matriz $\\hat{D}$ que es una matriz diagonal donde $D_{ii}$ contiene el número de vecinos que tiene el nodo $i$. Por último, $\\sigma$ representa una función de activación arbitraria, y no necesariamente la sigmoidea (normalmente en las GNNs se utiliza una función de activación basada en ReLU).\n",
    "\n",
    "Al implementar la capa GCN en PyTorch, podemos aprovechar la flexibilidad de las operaciones sobre tensores. En lugar de definir una matriz $\\hat{D}$, podemos simplemente dividir los mensajes sumados por el número de vecinos después. Además, sustituimos la matriz de pesos por una capa lineal, que adicionalmente nos permite añadir un sesgo.\n",
    "El código de la capa GCN se debe definir de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "VX3f8Vq6o0Dw"
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Tensor con las características del nodo con formato [batch_size, num_nodes, c_in].\n",
    "            adj_matrix - Matriz de adyacencia del grafo. Si hay una arista del nodo i al j, adj_matrix[b,i,j]=1 sino 0.\n",
    "                         También admite grafos dirigidos, asumiendo que ya llevan la matriz identidad sumada en batch_size: [batch_size, num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        #Numero de vecinos (num_neighbours), en este caso, se refiere a los vecinos de entrada (in) en grafos dirigidos\n",
    "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
    "        node_feats = node_feats / num_neighbours\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6dKxbRgo0Dw"
   },
   "source": [
    "Para comprender mejor la capa GCN, lo podemos aplicar a nuestro grafo de ejemplo anterior. En primer lugar, especifiquemos algunas características de los nodos y la matriz de adyacencia con autoconexiones añadidas (self-loops):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D30KHuf6o0Dw",
    "outputId": "cb4ebab4-539b-4775-b9e9-2174410110e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características de los nodos:\n",
      " tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "\\Matriz de adyacencia:\n",
      " tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])\n",
    "\n",
    "print(\"Características de los nodos:\\n\", node_feats)\n",
    "print(\"\\Matriz de adyacencia:\\n\", adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeF2nuN3o0Dx"
   },
   "source": [
    "A continuación, vamos a aplicarle una capa GCN. Para simplificar, inicializamos la matriz de pesos lineal como una matriz de identidad para que las características de entrada sean iguales a los mensajes. Esto nos facilita la verificación de la operación de paso de mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uqu9h_po0Dx",
    "outputId": "2c0a7329-f0ef-4add-ecca-ac88686f61f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de adyacencia tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Características de entrada tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Características de salida tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "with torch.no_grad(): #Dentro de este bloque de código incluimos todo lo que queremos ejecutar sin cálculo de gradientes\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(\"Matriz de adyacencia\", adj_matrix)\n",
    "print(\"Características de entrada\", node_feats)\n",
    "print(\"Características de salida\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdxoQTA8o0Dx"
   },
   "source": [
    "Como podemos ver, los valores de salida del primer nodo son la media de sí mismo y del segundo nodo. Del mismo modo, podemos verificar todos los demás nodos. Sin embargo, en una GNN, también querríamos permitir el intercambio de características entre nodos más allá de sus vecinos. Esto se puede conseguir aplicando múltiples capas GCN, lo que nos da la disposición final de una GNN. La GNN puede construirse mediante una secuencia de capas GCN y no lineales como ReLU. Se puede ver en la imagen.\n",
    "\n",
    "<center width=\"100%\" style=\"padding: 10px\"><img src=\"https://theaisummer.com/static/cb743cf6762bf14d48c1548bf0f0fe1f/9a128/gnn.jpg\" width=\"600px\"></center>\n",
    "\n",
    "Sin embargo, un problema que podemos ver al observar el ejemplo anterior es que las características de salida de los nodos 3 y 4 son las mismas porque tienen los mismos nodos adyacentes (incluido él mismo). Por lo tanto, las capas GCN pueden hacer que la red olvide la información específica de cada nodo si sólo tomamos la media de todos los mensajes. Se han propuesto muchas mejoras posibles. Mientras que la opción más sencilla podría ser utilizar conexiones residuales, el enfoque más común es ponderar más las autoconexiones o definir una matriz de pesos separada para las autoconexiones. Alternativamente, podemos tratarlo con Graph Attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_cagrRUo0Dy"
   },
   "source": [
    "### Graph Attention\n",
    "\n",
    "Antes de nada, tenemos que explicar **¿Qué es la atención?** Se trata de un mecanismo de capas de redes neuronales basada en secuenciación. Se puede definir como una media ponderada de elementos de secuencia con los pesos calculados de forma dinámica en función de una consulta de entrada y elementos clave.\n",
    "El objetivo es hacer una media de las características de varios elementos. Sin embargo, en lugar de ponderar cada elemento por igual, queremos ponderarlos en función de sus valores reales. En otras palabras, queremos decidir dinámicamente a qué entradas queremos \"prestar más atención\" que a otras. Se organiza en cuatro partes:\n",
    "\n",
    "\n",
    "*   Consulta (query):  vector de características que describe lo que estamos buscando en la secuencia, es decir, a qué querríamos prestar atención.\n",
    "*   Claves (keys): para cada elemento de entrada, tenemos una clave que, de nuevo, es un vector de características. Este vector de características describe a grandes rasgos lo que el elemento \"ofrece\", o cuándo puede ser importante. Las claves deben diseñarse de forma que podamos identificar los elementos a los que queremos prestar atención basándonos en la consulta.\n",
    "*   Valores (values): para cada elemento de entrada, también tenemos un vector de valores. Este vector de características es el que queremos promediar.\n",
    "*   Función de puntuación (score function): para calificar los elementos a los que queremos prestar atención, necesitamos especificar una función de puntuación, la cual toma como entrada la consulta y una clave, y da como salida la puntuación/peso de atención del par consulta-clave. Suele implementarse mediante métricas de similitud sencillas.\n",
    "\n",
    "Los pesos de la media se calculan mediante un softmax sobre todas las salidas de la función de puntuación. Por lo tanto, asignamos un peso mayor a aquellos vectores de valores cuya clave correspondiente es más similar a la consulta.\n",
    "\n",
    "\n",
    "**¿Y si aplicamos este concepto a los grafos? ** Puede aplicarse de forma similar a los grafos, con las Graph Attention Network (GAT). De forma similar a la GCN, la capa de atención de grafos crea un mensaje para cada nodo utilizando una matriz lineal de capa/peso. Para la parte de atención, utiliza el mensaje del propio nodo como consulta, y los mensajes a promediar como claves y valores (nótese que esto también incluye el mensaje a sí mismo). La función de puntuación $f_{atcn}$ se implementa como un Multi-Layer Perceptron (MLP) de una capa que asigna la consulta y la clave a un único valor. El MLP tiene el siguiente aspecto:\n",
    "\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://pctg.net/wp-content/uploads/2023/07/1688971037_411_Todo-lo-que-necesitas-saber-sobre-Graph-Attention-Networks.png\" width=\"250px\"></center>\n",
    "\n",
    "Entremos con un poco de notación matemática:\n",
    "\n",
    "$h_i$ y $h_j$ son las característias originales de los nodos $i$ y $j$ respectivamente, y representa los mensajes de la capa con $\\mathbf{W}$ como matriz de pesos. $\\mathbf{a}$ es la matriz de pesos de la MLP, la cual tiene la forma $[1,2\\times d_{\\text{msg}}]$, y $\\alpha_{ij}$ el peso de atención final desde el nodo $i$ al $j$. El cálculo puede ser descrito mediante la siguiente fórmula:\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)\\right)}$$\n",
    "\n",
    "El operador $||$ representa la concatenación, y $\\mathcal{N}_i$ los índices de los vecinos del nodo $i$. Nótese que, a diferencia de la práctica habitual, aplicamos una no linealidad (LeakyReLU) antes del softmax sobre los elementos.Aunque a primera vista parece un cambio menor, es crucial para que la atención dependa de la entrada original. Vamos a eliminar la no linealidad por un momento, y tratar de simplificar la expresión:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\alpha_{ij} & = \\frac{\\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_j\\right]\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}\\left[\\mathbf{W}h_i||\\mathbf{W}h_k\\right]\\right)}\\\\[5pt]\n",
    "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i+\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
    "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,:d/2}\\mathbf{W}h_i\\right)\\cdot\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\[5pt]\n",
    "    & = \\frac{\\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_j\\right)}{\\sum_{k\\in\\mathcal{N}_i} \\exp\\left(\\mathbf{a}_{:,d/2:}\\mathbf{W}h_k\\right)}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "Podemos ver que sin la no linealidad, el término de atención con $h_i$ en realidad se anula a sí mismo, por lo que la atención es independiente del propio nodo. Por lo tanto, tendríamos el mismo problema que el GCN de crear las mismas características de salida para los nodos con los mismos vecinos. Por eso el LeakyReLU es crucial y añade cierta dependencia de $h_i$ a la atención.\n",
    "\n",
    "Una vez que obtenemos todos los factores de atención, podemos calcular las características de salida para cada nodo realizando la media ponderada:\n",
    "\n",
    "$$h_i'=\\sigma\\left(\\sum_{j\\in\\mathcal{N}_i}\\alpha_{ij}\\mathbf{W}h_j\\right)$$\n",
    "\n",
    "$\\sigma$ es aún no lineal, como una capa GCN. Visualmente, podríamos representar el paso completo del mensaje en una capa de atención.\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://mila.quebec/wp-content/uploads/2018/07/687474703a2f2f7777772e636c2e63616d2e61632e756b2f25374570763237332f696d616765732f6761742e6a7067-1024x669.jpeg\" width=\"400px\"></center>\n",
    "\n",
    "Para aumentar la expresividad de la red de atención gráfica propusieron ampliarla a múltiples cabezas, con $N$ capas de atención que se aplican en paralelo. En la imagen de arriba, se visualiza como tres colores diferentes de flechas (verde, azul y morado) se concatenan. La media sólo se aplica a la última capa de predicción de una red.\n",
    "\n",
    "Un ejemplo de implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TeZa7Oqio0Dy"
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimensionalidad de las características de entrada\n",
    "            c_out - Dimensionalidad de las características de salida\n",
    "            num_heads - Número de heads (para trabajar en paralelo). Las características de salida se reparten equitativamente por cada head si concat_heads=True.\n",
    "            concat_heads - Si es True, la salida de cada head se concatena en lugar de promediarse.\n",
    "            alpha - Pendiente negativa de la activación LeakyReLU.\n",
    "        \"\"\"\n",
    "        super().__init__() #Inicializamos\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0, \"El número de características de salida debe ser múltiplo del número de heads.\"\n",
    "            c_out = c_out // num_heads\n",
    "\n",
    "        # Sub-modulos y parámetros necesarios en la capa\n",
    "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # Uno por head\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Inicialización de la implementación original\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            node_feats - Características de entrada del nodo. Formato: [batch_size, c_in]\n",
    "            adj_matrix - Matriz de adyacencia con self-loops. Formato: [batch_size, num_nodes, num_nodes]\n",
    "            print_attn_probs - Si es True, los pesos de atención se muestran durante el paso forward (debugging)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1) #Se extraen datos del tensor de entrada\n",
    "\n",
    "        # Applicamos una capa lineal y ordenamos los nodos por head\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        # Tenemos que calcular los \"attention logits\" (valores de la capa de salida, previa a la función de activación) para cada arista de la matriz de adyacencia\n",
    "        # Se trata de algo muy costoso, puesto que hay que hacerlo para toda combinación posible\n",
    "        # Solución => Creamos un tensor [W*h_i||W*h_j] con i y j como índices de todas las aristas\n",
    "        edges = adj_matrix.nonzero(as_tuple=False) # Obtenemos los índices donde la matriz de adyacencia no es 0 (existencia de arista)\n",
    "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
    "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # Devuelve un tensor con node_feats_flat con las posiciones indexadas en dim=0\n",
    "\n",
    "        # Calculamos la salida de atención de la MLP (independiente para cada head)\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Se mapea la lista de valores de atención a una matriz de nuevo\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        # Se calcula la media de pesos de atención\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
    "        if print_attn_probs:\n",
    "            print(\"Probabilidades de atención\\n\", attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        # Si se desean concatenar los head, se hace un reshaping. En caso contrario tomamos la media\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npNXzrh1o0Dz"
   },
   "source": [
    "Una vez más, podemos aplicar la capa graph attention en nuestro grafo de ejemplo anterior para entender mejor la dinámica. Al igual que antes, la capa de entrada se inicializa como una matriz de identidad, pero ponemos $\\mathbf{a}$ como vector de números arbitrarios para obtener diferentes valores de atención. Utilizamos dos head para mostrar los mecanismos de atención, paralelos e independientes, que trabajan en la capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2kXc6Xlo0Dz",
    "outputId": "797e7112-50d4-4f31-f41f-9a82f3813c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades de atención\n",
      " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "Matriz de Adyacencia tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n",
      "Características de entrada tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "Características de salida tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GATLayer(2, 2, num_heads=2) #Dos dimensiones de entrada y salida, y dos head\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]]) #Los pesos es la matriz de identidad\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.]) #Sin sesgo\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]]) #Datos de ejemplo\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
    "\n",
    "print(\"Matriz de Adyacencia\", adj_matrix)\n",
    "print(\"Características de entrada\", node_feats)\n",
    "print(\"Características de salida\", out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkhAgHI9o0Dz"
   },
   "source": [
    "Recomendación: intentar calcular la matriz de atención al menos para una cabeza y un nodo por sí mismo. Las entradas son 0 cuando no existe ninguna arista entre $i$ y $j$. Para los demás, vemos un conjunto diverso de probabilidades de atención. Además, las características de salida de los nodos 3 y 4 son ahora diferentes aunque tengan los mismos vecinos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYti9Rc1o0D0"
   },
   "source": [
    "## PyTorch Geometric (Grandes grafos)\n",
    "\n",
    "Hemos probado con grafos de ejemplo, pero...**¿Qué ocurre con grafos del mundo real?** Ya hemos mencionado antes que la implementación de grafos con matrices de adyacencia es sencilla y directa, pero puede resultar costosa desde el punto de vista computacional para grafos de gran tamaño. Muchos grafos del mundo real pueden llegar a tener más de cientos de miles o millones nodos (por ejemplo, una red social), por lo que las implementaciones basadas en matrices de adyacencia fallan. Hay muchas optimizaciones posibles al implementar GNNs, y por suerte, existen paquetes que proporcionan dichas capas. Los paquetes más populares para PyTorch son [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) y [Deep Graph Library](https://www.dgl.ai/) (esta última no está específicamente diseñado para funcionar con un framework o entorno de desarrollo particular). Cuál usar depende del proyecto que estemos planeando hacer. Lo primero que vamos a hacer es instalarlo (PyTorch Geometric no está instalado por defecto en GoogleColab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "fLdzlvtuo0D0"
   },
   "outputs": [],
   "source": [
    "# torch geometric\n",
    "try:\n",
    "    import torch_geometric\n",
    "except ModuleNotFoundError:\n",
    "    # Instalar paquetes torch geometriccon una versión específica de CUDA+PyTorch.\n",
    "    # Más detalles en https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html\n",
    "    TORCH = torch.__version__.split('+')[0]\n",
    "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
    "\n",
    "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "    !pip install torch-geometric\n",
    "    import torch_geometric\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5fFQndRo0D0"
   },
   "source": [
    "PyTorch Geometric nos proporciona un conjunto de capas de grafos comunes, incluyendo las capas GCN y GAT implementadas antes. Además, de forma similar a torchvision de PyTorch, proporciona los conjuntos de datos de grafos comunes y las transformaciones en ellos para simplificar el entrenamiento. En comparación con nuestra implementación anterior, PyTorch Geometric utiliza una lista de pares de índices para representar las aristas. Los detalles de esta biblioteca se explorarán más a fondo en nuestros experimentos.\n",
    "\n",
    "En nuestras tareas a continuación, queremos que nos permita elegir entre una multitud de capas de grafos. Por lo tanto, definimos de nuevo a continuación un diccionario para acceder usando strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bSV1AW5to0D1"
   },
   "outputs": [],
   "source": [
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \"GraphConv\": geom_nn.GraphConv\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geRDbAc8o0D1"
   },
   "source": [
    "Además de las GCN y GAT explicadas, hemos includo la capa `geom_nn.GraphConv` ([documentación](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv)). GraphConv es un GCN con una matriz de pesos separada para las autoconexiones. Matemáticamente sería así:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^{(l+1)} = \\mathbf{W}^{(l + 1)}_1 \\mathbf{x}_i^{(l)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{j \\in \\mathcal{N}_i} \\mathbf{x}_j^{(l)}\n",
    "$$\n",
    "\n",
    "En esta fórmula, los mensajes de los vecinos se suman en lugar de promediarse. Sin embargo, PyTorch Geometric proporciona el argumento `aggr` para cambiar entre sumar, promediar y max pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mt9dcea1hW2"
   },
   "source": [
    "# **TAREA 1b: Graph Neural Networks (Práctica)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWQ8Ag6wo0D1"
   },
   "source": [
    "## Experimentos con datos estructurados en grafos\n",
    "\n",
    "Las tareas sobre datos estructurados en grafos pueden agruparse en tres grupos: a nivel de nodo, a nivel de arista y a nivel de grafo. Los distintos niveles describen en qué nivel queremos realizar la clasificación/regresión. A continuación analizaremos los tres tipos con ejemplos concretos, con más detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml1-ga1Oo0D1"
   },
   "source": [
    "### A nivel de nodo (Node-level tasks). Ejemplo: clasificación semisupervisada de publicaciones científicas.\n",
    "\n",
    "\n",
    "Las tareas a nivel de nodo tienen como objetivo clasificar los nodos de un grafo. Normalmente, tenemos un único grafo de gran tamaño con más de 1.000 nodos, de los cuales una cierta cantidad están etiquetados. Aprendemos a clasificar esos ejemplos etiquetados durante el entrenamiento e intentamos generalizar a los nodos no etiquetados.\n",
    "\n",
    "Un ejemplo típico es el dataset  [Cora](https://relational.fit.cvut.cz/dataset/CORA), que se trata de una red de citas entre artículos científicos. Cora consta de 2708 publicaciones científicas con 5429 enlaces entre sí que representan la citación de un artículo por otro. La tarea consiste en clasificar cada publicación en una de las siete clases o categorías (Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning y Theory).\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"https://production-media.paperswithcode.com/datasets/Cora-0000000700-ce1c5ec7_LD7pZnT.jpg\" width=\"350px\"></center>\n",
    "\n",
    "\n",
    "Cada publicación está representada por un vector bag-of-words. Esto significa que tenemos un vector de 1433 elementos para cada publicación, donde un 1 en la característica $i$ indica que la $i$-ésima palabra de un diccionario predefinido está en el artículo. Las representaciones binarias de bag-of-words se suelen utilizar cuando necesitamos codificaciones muy sencillas y ya tenemos una intuición de qué palabras esperar en una red. Existen otros enfoques mejores, que se verán más adelante.\n",
    "\n",
    "A continuación cargaremos el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3maF58aVo0D2"
   },
   "outputs": [],
   "source": [
    "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ-_zmIRo0D2"
   },
   "source": [
    "Vamos a ver cómo PyTorch Geometric representa los datos de grafo. Hay que tener en cuenta que, a pesar de que tenemos un solo grafo, PyTorch Geometric devuelve un conjunto de datos para la compatible con otros datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgFy7Ok3o0D2",
    "outputId": "330ec467-fd17-4a4e-d736-3a3348069a20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cora_dataset[0]\n",
    "data # x es una matriz de características con formato [num_nodes, num_node_features]\n",
    "#data.is_directed()\n",
    "#data.num_edges # ¿Por qué sale este número de aristas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvUysNDUo0D3"
   },
   "source": [
    "El grafo está representado por un objeto `Data` ([documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)) al que podemos acceder como un namespace estándar de Python. El tensor de índices de aristas es la lista de aristas del grafo y contiene la versión simétrica de cada arista para grafos no dirigidos. Las máscaras `train_mask`, `val_mask` y `test_mask` son máscaras booleanas que indican qué nodos debemos utilizar para el entrenamiento, la validación y la prueba. El tensor `x` es el tensor de características de nuestras 2708 publicaciones, y `y` las etiquetas de todos los nodos.\n",
    "\n",
    "Una vez vistos los datos, podemos implementar una red neuronal de grafos sencilla. La GNN aplica una secuencia de capas de grafos (GCN, GAT o GraphConv), ReLU como función de activación y dropout para la regularización. Vamos a ver la implementación específica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "RqQmkcv3o0D3"
   },
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Entradas:\n",
    "            c_in - Dimensionalidad de las características de entrada\n",
    "            c_hidden - Dimensionalidad de las características de la oculta\n",
    "            c_out - Dimensionalidad de las características de salida. Normalmente corresponde con el número de clases de la clasificación\n",
    "            num_layers - Número de capas ocultas\n",
    "            layer_name - Nombre de técnica a usar\n",
    "            dp_rate - La \"tasa de dropout\" (dropout rate) de la red (hiperparámetro para regularizar y mejorar el rendimiento, reduciendo así el riesgo de overfitting)\n",
    "            kwargs - Argumentos adicionales (por ejemplo, el número de heads su usamos una GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels,\n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Entradas:\n",
    "            x - Características de entrada para cada nodo\n",
    "            edge_index - Lista de índices de aristas del grafo (notación de PyTorch geometric)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # Para las capas, necesitamos añadir el tensor \"edge_index\" como entrada adicional\n",
    "            # Toda capa PyTorch Geometric graph hereda la clase \"MessagePassing\", por lo tanto, podemos comprobar el tipo de clase de forma sencilla\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HODOfwpco0D4"
   },
   "source": [
    "Una buena práctica en las tareas a nivel de nodo consiste en crear una MLP baseline que se aplique a cada nodo de forma independiente. De esta forma podemos comprobar si la suma de la información del grafo al modelo mejora realmente la predicción, o no. También puede ocurrir que las características por nodo ya sean lo suficientemente expresivas como para apuntar claramente hacia una clase específica. Para comprobarlo, a continuación implementamos un MLP sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "OXgzrDjBo0D4"
   },
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Entradas:\n",
    "            c_in - Dimensionalidad de las características de entrada\n",
    "            c_hidden - Dimensionalidad de las características de la oculta\n",
    "            c_out - Dimensionalidad de las características de salida. Normalmente corresponde con el número de clases de la clasificación\n",
    "            num_layers - Número de capas ocultas\n",
    "            dp_rate - La \"tasa de dropout\" de la red\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1): #Aplicamos linealidad, funcion de activación RELU y dropout para el overfitting\n",
    "            layers += [\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entradas:\n",
    "            x - Características de entrada por nodo\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQX1XQX3o0D5"
   },
   "source": [
    "Por último, podemos fusionar los modelos en un módulo PyTorch Lightning que se encargue del entrenamiento, la validación y las pruebas por nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "mgAhRT-2o0D6"
   },
   "outputs": [],
   "source": [
    "class NodeLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Guardar los hiperparámetros\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            self.model = MLPModel(**model_kwargs)\n",
    "        else:\n",
    "            self.model = GNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Solo se calcula la pérdida en los nodos que corresponden a la máscara\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Modo desconocido: {mode}\"\n",
    "\n",
    "        loss = self.loss_module(x[mask], data.y[mask]) #Pérdida\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum() #Eficiencia (accuracy)\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Usamos Stochastic Gradient Descent (SGD) como algoritmo de optimización, pero Adam también es una buena opción\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW4ptUqYo0D6"
   },
   "source": [
    "Además del módulo Lightning, definimos una función de entrenamiento. Como tenemos un grafo simple, utilizamos un tamaño de lote (batch_size) de 1 para cargar datos y compartimos el mismo para el conjunto de entrenamiento, validación y prueba (la máscara se elige dentro del módulo Lightning). Además, establecemos el argumento `enable_progress_bar` a False (sirve para mostrar una barra de progreso con el progreso del entrenamiento a medida que se ejecutan las épocas, pero en este caso al tener tamaño uno carece de sentido, y se muestra solo al final)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "QutHVL6do0D6"
   },
   "outputs": [],
   "source": [
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1) #loader.DataLoader\n",
    "\n",
    "    # Creamos un PyTorch Lightning trainer (entrenamiento) con llamada de retorno (generation callback)\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=200,\n",
    "                         enable_progress_bar=False) # False porque el tamaño de la época es 1\n",
    "    trainer.logger._default_hp_metric = None # No es necesario un argumento para el logging\n",
    "\n",
    "    # Comprobamos que el modelo preentrenado existe, cargándolo y saltando el paso de entrenamiento en caso afirmativo\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Se ha encontrado un modelo preentrenado, cargando...\")\n",
    "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything()\n",
    "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
    "        trainer.fit(model, node_data_loader, node_data_loader)\n",
    "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Probamos el mejor modelo en el conjunto de test\n",
    "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "    result = {\"train\": train_acc,\n",
    "              \"val\": val_acc,\n",
    "              \"test\": test_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5CuOi7eo0D7"
   },
   "source": [
    "Por último, podemos entrenar nuestros modelos. En primer lugar, vamos a entrenar el MLP simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "87_a5yvco0D7"
   },
   "outputs": [],
   "source": [
    "#Función para mostrar los resultados\n",
    "def print_results(result_dict):\n",
    "    print(f\"\\nAccuracy:\\n\")\n",
    "    if \"train\" in result_dict:\n",
    "        print(f\"Entrenamiento: {(100.0*result_dict['train']):4.2f}%\")\n",
    "    if \"val\" in result_dict:\n",
    "        print(f\"Validación:   {(100.0*result_dict['val']):4.2f}%\")\n",
    "    print(f\"Test:  {(100.0*result_dict['test']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xUepWOQo0D8",
    "outputId": "7101f675-0017-4942-ae5f-9d6665c3e33d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | MLPModel         | 23.1 K\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "23.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.1 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "\n",
      "Entrenamiento: 97.14%\n",
      "Validación:   50.40%\n",
      "Test:  60.60%\n"
     ]
    }
   ],
   "source": [
    "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
    "                                                        dataset=cora_dataset,\n",
    "                                                        c_hidden=16,\n",
    "                                                        num_layers=2,\n",
    "                                                        dp_rate=0.1)\n",
    "\n",
    "print_results(node_mlp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ph3U2OQObvW"
   },
   "source": [
    "**Pregunta 1:** ¿Cómo interpretas los valores de accuracy de los diferentes conjuntos? Explicación detallada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xc1CrfNo0D8"
   },
   "source": [
    "Ahora probamos con la GNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5OD__xBo0D9",
    "outputId": "5d46cb2c-f478-4034-b77a-81fa056896e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | GNNModel         | 23.1 K\n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "23.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.1 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=200` reached.\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "\n",
      "Entrenamiento: 100.00%\n",
      "Validación:   76.20%\n",
      "Test:  81.10%\n"
     ]
    }
   ],
   "source": [
    "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
    "                                                        layer_name=\"GCN\",\n",
    "                                                        dataset=cora_dataset,\n",
    "                                                        c_hidden=16,\n",
    "                                                        num_layers=2,\n",
    "                                                        dp_rate=0.1)\n",
    "print_results(node_gnn_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3M2E-FPP7O5"
   },
   "source": [
    "**Pregunta 2:** ¿Cómo interpretas los valores de accuracy de los diferentes conjuntos ahora? Explicación detallada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5lw1Q4uQZY-"
   },
   "source": [
    "**Conclusión** Redactar la conclusión que se obtiene de esta red Cora con este modelo. Probar modificaciones para comparar y obtener mejores (o peores) resultados, y explicarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1sBtVhZo0D-"
   },
   "source": [
    "### A nivel de aristas (Edge-level tasks). Ejemplo:  Predicción de enlaces\n",
    "\n",
    "En algunas aplicaciones, es posible que tengamos que predecir a nivel de arista en lugar de a nivel de nodo. La tarea a nivel de arista más común en GNN es la predicción de enlaces (link prediction). Significa que, dado un grafo, queremos predecir si habrá o debería haber una arista o enlace entre dos nodos. Por ejemplo, en una red social, para proponerte nuevos amigos. De nuevo, la información a nivel de grafo puede ser crucial para realizar esta tarea. La predicción del resultado se realiza normalmente aplicando una métrica de similitud sobre un par de características de los nodos, que debería ser 1 si existe relación y 0 en caso contrario.\n",
    "\n",
    "Siguiendo el mismo ejemplo que antes (Cora dataset), ahora no vamos a clasificar de que tipo es cada publicación científica, sino vamos a predecir que artículos deberían citarse entre sí (link prediction). En la práctica, es añadir nuevas aristas al grafo. Para ello, vamos a usar el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUkAylWaG_3T",
    "outputId": "ba30f86c-0c00-46d5-916a-d10e53ac0d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6930, Val: 0.6883, Test: 0.7006\n",
      "Epoch: 002, Loss: 0.6810, Val: 0.6788, Test: 0.6931\n",
      "Epoch: 003, Loss: 0.7201, Val: 0.6833, Test: 0.6944\n",
      "Epoch: 004, Loss: 0.6768, Val: 0.7009, Test: 0.7056\n",
      "Epoch: 005, Loss: 0.6854, Val: 0.7351, Test: 0.7299\n",
      "Epoch: 006, Loss: 0.6896, Val: 0.7641, Test: 0.7691\n",
      "Epoch: 007, Loss: 0.6910, Val: 0.7278, Test: 0.7385\n",
      "Epoch: 008, Loss: 0.6912, Val: 0.6981, Test: 0.7059\n",
      "Epoch: 009, Loss: 0.6907, Val: 0.6905, Test: 0.7008\n",
      "Epoch: 010, Loss: 0.6893, Val: 0.6915, Test: 0.7052\n",
      "Epoch: 011, Loss: 0.6864, Val: 0.6943, Test: 0.7093\n",
      "Epoch: 012, Loss: 0.6825, Val: 0.6948, Test: 0.7093\n",
      "Epoch: 013, Loss: 0.6797, Val: 0.6958, Test: 0.7059\n",
      "Epoch: 014, Loss: 0.6799, Val: 0.6957, Test: 0.7001\n",
      "Epoch: 015, Loss: 0.6766, Val: 0.6936, Test: 0.6944\n",
      "Epoch: 016, Loss: 0.6718, Val: 0.6925, Test: 0.6899\n",
      "Epoch: 017, Loss: 0.6665, Val: 0.6959, Test: 0.6902\n",
      "Epoch: 018, Loss: 0.6623, Val: 0.6973, Test: 0.6912\n",
      "Epoch: 019, Loss: 0.6562, Val: 0.6975, Test: 0.6893\n",
      "Epoch: 020, Loss: 0.6492, Val: 0.6996, Test: 0.6903\n",
      "Epoch: 021, Loss: 0.6421, Val: 0.7115, Test: 0.7036\n",
      "Epoch: 022, Loss: 0.6375, Val: 0.7356, Test: 0.7353\n",
      "Epoch: 023, Loss: 0.6254, Val: 0.7508, Test: 0.7625\n",
      "Epoch: 024, Loss: 0.6155, Val: 0.7490, Test: 0.7708\n",
      "Epoch: 025, Loss: 0.6020, Val: 0.7498, Test: 0.7720\n",
      "Epoch: 026, Loss: 0.5894, Val: 0.7522, Test: 0.7696\n",
      "Epoch: 027, Loss: 0.5847, Val: 0.7483, Test: 0.7697\n",
      "Epoch: 028, Loss: 0.5736, Val: 0.7424, Test: 0.7693\n",
      "Epoch: 029, Loss: 0.5687, Val: 0.7430, Test: 0.7702\n",
      "Epoch: 030, Loss: 0.5674, Val: 0.7431, Test: 0.7713\n",
      "Epoch: 031, Loss: 0.5659, Val: 0.7393, Test: 0.7755\n",
      "Epoch: 032, Loss: 0.5609, Val: 0.7393, Test: 0.7803\n",
      "Epoch: 033, Loss: 0.5596, Val: 0.7465, Test: 0.7893\n",
      "Epoch: 034, Loss: 0.5531, Val: 0.7538, Test: 0.7982\n",
      "Epoch: 035, Loss: 0.5477, Val: 0.7592, Test: 0.8062\n",
      "Epoch: 036, Loss: 0.5440, Val: 0.7639, Test: 0.8121\n",
      "Epoch: 037, Loss: 0.5352, Val: 0.7690, Test: 0.8169\n",
      "Epoch: 038, Loss: 0.5348, Val: 0.7740, Test: 0.8213\n",
      "Epoch: 039, Loss: 0.5232, Val: 0.7790, Test: 0.8254\n",
      "Epoch: 040, Loss: 0.5187, Val: 0.7835, Test: 0.8283\n",
      "Epoch: 041, Loss: 0.5162, Val: 0.7903, Test: 0.8330\n",
      "Epoch: 042, Loss: 0.5134, Val: 0.7973, Test: 0.8379\n",
      "Epoch: 043, Loss: 0.5076, Val: 0.8031, Test: 0.8427\n",
      "Epoch: 044, Loss: 0.5113, Val: 0.8063, Test: 0.8458\n",
      "Epoch: 045, Loss: 0.5065, Val: 0.8110, Test: 0.8482\n",
      "Epoch: 046, Loss: 0.5047, Val: 0.8134, Test: 0.8508\n",
      "Epoch: 047, Loss: 0.5080, Val: 0.8151, Test: 0.8523\n",
      "Epoch: 048, Loss: 0.5073, Val: 0.8157, Test: 0.8523\n",
      "Epoch: 049, Loss: 0.5084, Val: 0.8153, Test: 0.8517\n",
      "Epoch: 050, Loss: 0.5012, Val: 0.8163, Test: 0.8527\n",
      "Epoch: 051, Loss: 0.4936, Val: 0.8186, Test: 0.8561\n",
      "Epoch: 052, Loss: 0.4955, Val: 0.8208, Test: 0.8585\n",
      "Epoch: 053, Loss: 0.4929, Val: 0.8215, Test: 0.8589\n",
      "Epoch: 054, Loss: 0.4956, Val: 0.8205, Test: 0.8578\n",
      "Epoch: 055, Loss: 0.4905, Val: 0.8234, Test: 0.8590\n",
      "Epoch: 056, Loss: 0.4935, Val: 0.8259, Test: 0.8618\n",
      "Epoch: 057, Loss: 0.4881, Val: 0.8293, Test: 0.8654\n",
      "Epoch: 058, Loss: 0.4872, Val: 0.8314, Test: 0.8685\n",
      "Epoch: 059, Loss: 0.4946, Val: 0.8335, Test: 0.8702\n",
      "Epoch: 060, Loss: 0.4847, Val: 0.8388, Test: 0.8719\n",
      "Epoch: 061, Loss: 0.4832, Val: 0.8427, Test: 0.8744\n",
      "Epoch: 062, Loss: 0.4786, Val: 0.8482, Test: 0.8801\n",
      "Epoch: 063, Loss: 0.4774, Val: 0.8520, Test: 0.8841\n",
      "Epoch: 064, Loss: 0.4739, Val: 0.8564, Test: 0.8860\n",
      "Epoch: 065, Loss: 0.4616, Val: 0.8600, Test: 0.8863\n",
      "Epoch: 066, Loss: 0.4628, Val: 0.8628, Test: 0.8865\n",
      "Epoch: 067, Loss: 0.4686, Val: 0.8658, Test: 0.8869\n",
      "Epoch: 068, Loss: 0.4663, Val: 0.8691, Test: 0.8893\n",
      "Epoch: 069, Loss: 0.4665, Val: 0.8720, Test: 0.8905\n",
      "Epoch: 070, Loss: 0.4722, Val: 0.8741, Test: 0.8900\n",
      "Epoch: 071, Loss: 0.4689, Val: 0.8745, Test: 0.8893\n",
      "Epoch: 072, Loss: 0.4613, Val: 0.8761, Test: 0.8909\n",
      "Epoch: 073, Loss: 0.4596, Val: 0.8782, Test: 0.8939\n",
      "Epoch: 074, Loss: 0.4673, Val: 0.8791, Test: 0.8946\n",
      "Epoch: 075, Loss: 0.4600, Val: 0.8795, Test: 0.8957\n",
      "Epoch: 076, Loss: 0.4596, Val: 0.8789, Test: 0.8962\n",
      "Epoch: 077, Loss: 0.4547, Val: 0.8789, Test: 0.8979\n",
      "Epoch: 078, Loss: 0.4561, Val: 0.8786, Test: 0.8993\n",
      "Epoch: 079, Loss: 0.4523, Val: 0.8777, Test: 0.8995\n",
      "Epoch: 080, Loss: 0.4561, Val: 0.8759, Test: 0.8977\n",
      "Epoch: 081, Loss: 0.4598, Val: 0.8756, Test: 0.8964\n",
      "Epoch: 082, Loss: 0.4568, Val: 0.8770, Test: 0.8975\n",
      "Epoch: 083, Loss: 0.4619, Val: 0.8781, Test: 0.8994\n",
      "Epoch: 084, Loss: 0.4527, Val: 0.8780, Test: 0.9007\n",
      "Epoch: 085, Loss: 0.4548, Val: 0.8784, Test: 0.9005\n",
      "Epoch: 086, Loss: 0.4533, Val: 0.8791, Test: 0.8995\n",
      "Epoch: 087, Loss: 0.4521, Val: 0.8800, Test: 0.8988\n",
      "Epoch: 088, Loss: 0.4526, Val: 0.8813, Test: 0.8995\n",
      "Epoch: 089, Loss: 0.4506, Val: 0.8817, Test: 0.9009\n",
      "Epoch: 090, Loss: 0.4488, Val: 0.8821, Test: 0.9015\n",
      "Epoch: 091, Loss: 0.4458, Val: 0.8828, Test: 0.9009\n",
      "Epoch: 092, Loss: 0.4587, Val: 0.8847, Test: 0.9008\n",
      "Epoch: 093, Loss: 0.4483, Val: 0.8864, Test: 0.9012\n",
      "Epoch: 094, Loss: 0.4553, Val: 0.8866, Test: 0.9015\n",
      "Epoch: 095, Loss: 0.4498, Val: 0.8850, Test: 0.9018\n",
      "Epoch: 096, Loss: 0.4469, Val: 0.8842, Test: 0.9015\n",
      "Epoch: 097, Loss: 0.4514, Val: 0.8850, Test: 0.9027\n",
      "Epoch: 098, Loss: 0.4496, Val: 0.8855, Test: 0.9026\n",
      "Epoch: 099, Loss: 0.4446, Val: 0.8847, Test: 0.9020\n",
      "Epoch: 100, Loss: 0.4538, Val: 0.8843, Test: 0.9025\n",
      "Final Test: 0.9015\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#Definimos una serie de transformaciones secuencialmente y ordenado.\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(), #Normalización de características en un conjunto de datos\n",
    "    T.ToDevice(device), #Transferimos los tensores o modelos a un dispositivo de hardware específico definido en \"device\"\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, #Realiza una división aleatoria de los datos a nivel de aristas en tres conjuntos de entrenamiento, validación y test\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "#Cargamos el dataset, aplicando las transformaciones\n",
    "cora_dataset = Planetoid(root=DATASET_PATH, name='Cora', transform=transform)\n",
    "#Como hemos dividido el dataset en tres conjuntos con RandomLinkSplit, los datos están en una lista de tuplas (train_data, val_data, test_data)\n",
    "train_data, val_data, test_data = cora_dataset[0]\n",
    "\n",
    "#Net sirve para definir modelos en PyTorch, heredando de torch.nn.Module. Obtenemos un modelo personalizado que se puede utilizar para definir arquitecturas de redes neuronales\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "#Creamos el modelo personalizado con nuestro dataset, y definimos el optimizador Adam y la función de perdida BCEWithLogitsLoss\n",
    "model = Net(cora_dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Entrenamiento de la red\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad() #Reseteo de los gradientes a cero antes de entrenar\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    #Realizamos una nueva ronda de muestreo negativo para cada época de entrenamiento\n",
    "    #Sirve para abordar el problema de la eficiencia computacional al trabajar con grandes conjuntos de datos.\n",
    "    #La técnica se utiliza para reducir el número de ejemplos de entrenamiento, lo que acelera significativamente el proceso de entrenamiento sin sacrificar la calidad del modelo\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "#Prueba de los datos, obteniendo el resultado con la medida ROC AUC\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101): #Usamos 100 épocas\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unTCuFUOPPAx"
   },
   "source": [
    "**Pregunta 3:** ¿Qué es el ROC AUC score? ¿Para qué sirve? ¿En qué casos es mejor usar accuracy y en cuales AUC? Modificar el código para obtener los resultados con accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xal23Mp4QJP6"
   },
   "source": [
    "**Ejercicio 4:** Modificar el código y realizar una gráfica de cómo van evolucionando los valores en los diferentes conjuntos a lo largo de las épocas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSInVvkfo0D-"
   },
   "source": [
    "### A nivel de grafo (graph-level tasks). Ejemplo: Clasificación de moléculas\n",
    "\n",
    "Por último, vamos a aplicar las GNN a una clasificación a nivel de grafos. El objetivo es clasificar un grafo entero en lugar de nodos o aristas individuales. Por ejemplo, imaginemos que queremos comparar si una red de recomendaciones es muy parecida a la de otro cliente, para clasificarlos como similares de cara a futuras recomendaciones.\n",
    "\n",
    "Como ejemplo práctico, vamos a usar un dataset con múltiples grafos que tenemos que clasificar en base a algunas propiedades estructurales del grafo. Lo más típico es la predicción de propiedades moleculares, en la que las moléculas se representan como grafos. Cada átomo está vinculado a un nodo, y las aristas del grafo son los enlaces entre átomos. Podemos ver el ejemplo en la figura:\n",
    "\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://phlippe.github.io/post/categorical-nf/molecule_graph.svg\" width=\"600px\"></center>\n",
    "\n",
    "A la izquierda, tenemos una pequeña molécula con diferentes átomos, mientras que la parte derecha de la imagen muestra la representación en forma de grafo. Los tipos de átomo se abstraen como características de nodo (por ejemplo, un vector de un punto), y los diferentes tipos de enlace se utilizan como características de la arista. Por simplicidad,no vamos a tener en cuenta los atributos de las aristas, pero se pueden incluir con métodos como la Convolución Relacional de Grafos (Relational Graph Convolution) que utiliza una matriz de pesos diferente para cada tipo de borde.\n",
    "\n",
    "\n",
    "El dataset a usar se llama [MUTAG](https://paperswithcode.com/dataset/mutag). Es una colección de compuestos nitroaromáticos y el objetivo es predecir su mutagenicidad en Salmonella typhimurium. Contiene 188 compuestos químicos/grafos con 18 nodos y 20 aristas de media para cada grafo. Los nodos del grafo tienen 7 etiquetas/tipos de átomo diferentes, y las etiquetas binarias del grafo representan \"su efecto mutagénico en una bacteria gramnegativa específica\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzVCHtCeo0D-",
    "outputId": "195362f3-04b5-4f68-fc29-22b2b0cb402e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Extracting ../data/MUTAG/MUTAG.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mutag_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8VnZPCWo0D_"
   },
   "source": [
    "Vemos algunas estadísticas del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8x_9T6Kao0D_",
    "outputId": "987384ec-9faf-4acf-b454-87a654ae5199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato de los datos: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
      "Número de grafos: 188\n",
      "Etiqueta media: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"Formato de los datos:\", mutag_dataset.data)\n",
    "print(\"Número de grafos:\", len(mutag_dataset))\n",
    "print(f\"Etiqueta media: {mutag_dataset.data.y.float().mean().item():4.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC3LhHCzo0D_"
   },
   "source": [
    "La primera línea muestra cómo el conjunto de datos almacena los distintos grafos en un tensor, con la concatenación de los nodos, las aristas y las etiquetas de cada grafo, así los índices en los que se dividen los tensores.\n",
    "\n",
    "La longitud del conjunto de datos es el número de grafos que tenemos, y la \"etiqueta media\" denota el porcentaje del grafo con etiqueta 1. Mientras el porcentaje esté en el intervalo de 0,5, tendremos un conjunto de datos relativamente equilibrado. Es una práctica muy recomendada, puesto que, a menudo, los conjuntos de datos de grafos están muy desequilibrados.\n",
    "\n",
    "Ahora vamos a dividir nuestro conjunto de datos en una parte de entrenamiento y otra de prueba. Tenga en cuenta que esta vez no utilizamos un conjunto de validación debido al pequeño tamaño del conjunto de datos. Por lo tanto, nuestro modelo podría sobreajustarse ligeramente en el conjunto de validación debido al ruido de la evaluación, pero aún así obtendremos una estimación del rendimiento en datos no entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JOyvoK1so0D_"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "mutag_dataset.shuffle()\n",
    "train_dataset = mutag_dataset[:150]\n",
    "test_dataset = mutag_dataset[150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG0qUJsQo0EA"
   },
   "source": [
    "Cuando usamos un data loader, nos encontramos con un problema al agrupar $N$ grafos por lotes. Cada grafo del lote puede tener un número diferente de nodos y aristas, por lo que necesitaríamos mucho relleno para obtener un único tensor.\n",
    "\n",
    "Torch geometric utiliza un enfoque diferente y más eficiente: podemos ver los grafos $N$ de un lote como un único grafo grande con una lista concatenada de nodos y aristas. Como no hay aristas entre los grafos $N$, ejecutar capas GNN en el grafo grande nos da el mismo resultado que ejecutar el GNN en cada grafo por separado (ver imagen abajo).\n",
    "\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://theaisummer.com/static/4e7a74f1f1559f684294e111b7742d11/105d8/graph-batching.png\" width=\"600px\"></center>\n",
    "\n",
    "La matriz de adyacencia es cero para los nodos que proceden de dos grafos diferentes y, en caso contrario, según la matriz de adyacencia del grafo individual. Por suerte, esta estrategia ya está implementada en torch geometric, por lo que podemos usar el data loader correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tjpgq4LDo0EA",
    "outputId": "32422a5e-9b10-4d0b-ad30-decded009317"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True) #Shuffle = organizar en cada época\n",
    "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64)\n",
    "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1TrGL2Co0EA"
   },
   "source": [
    "Carguemos un lote a continuación para ver la agrupación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRVnCBIco0EA",
    "outputId": "44c1d92a-9f49-49d2-e7e6-02c9353cb623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
      "Etiquetas: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
      "Índices del batch: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(graph_test_loader))\n",
    "print(\"Batch:\", batch)\n",
    "print(\"Etiquetas:\", batch.y[:10])\n",
    "print(\"Índices del batch:\", batch.batch[:40]) #Visualizamos solo las 40 primeras, para entenderlo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYL8Xtl1o0EB"
   },
   "source": [
    "Tenemos 38 grafos agrupados juntos para el conjunto de datos de test. Los índices de lote, almacenados en `batch`, muestran que los 12 primeros nodos pertenecen al primer grafo, los 22 siguientes al segundo, y así sucesivamente. Estos índices son importantes para realizar la predicción final. Para realizar una predicción sobre un grafo completo, normalmente realizamos una operación de agrupación sobre todos los nodos después de ejecutar el modelo GNN. En este caso, utilizaremos el average pooling. Por lo tanto, necesitamos saber qué nodos deben incluirse en cada average pool. Usando este pooling, ya podemos crear nuestra red de abajo. Concretamente, reutilizamos nuestra clase `GNNModel` de antes, y simplemente añadimos un average pooling y una única capa lineal para la tarea de predicción de grafos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "r28ct0ovo0EB"
   },
   "outputs": [],
   "source": [
    "class GraphGNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Entradas:\n",
    "            c_in - Dimensionalidad de las características de entrada\n",
    "            c_hidden - Dimensionalidad de las características de la oculta\n",
    "            c_out - Dimensionalidad de las características de entrada (normalmente el número de clases)\n",
    "            dp_rate_linear - La tasa de dropout antes de la capa lineal (normalmente mucho mayor que dentro de la GNN)\n",
    "            kwargs - Argumentos adicionales para la GNN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.GNN = GNNModel(c_in=c_in,\n",
    "                            c_hidden=c_hidden,\n",
    "                            c_out=c_hidden, #No sería la predicción final todavia\n",
    "                            **kwargs)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),\n",
    "            nn.Linear(c_hidden, c_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Entradas:\n",
    "            x - Características de entrada por nodo\n",
    "            edge_index - Lista de índices de los nodos que estan conectados por aristas en el grafo (notación de PyTorch geometric)\n",
    "            batch_idx - Índice del lote para cada nodo\n",
    "        \"\"\"\n",
    "        x = self.GNN(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39MBUQoXo0EB"
   },
   "source": [
    "Por último, podemos crear un módulo PyTorch Lightning para manejar el entrenamiento, como ya explicamos antes. Como tenemos una tarea de clasificación binaria, usamos la función de pérdida Binary Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GcyPo_FKo0EB"
   },
   "outputs": [],
   "source": [
    "class GraphLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Guardando los hiperparámetros\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss() #Función de pérdida\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        x = x.squeeze(dim=-1)\n",
    "\n",
    "        if self.hparams.c_out == 1:\n",
    "            preds = (x > 0).float()\n",
    "            data.y = data.y.float()\n",
    "        else:\n",
    "            preds = x.argmax(dim=-1)\n",
    "        loss = self.loss_module(x, data.y)\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) #Se pone una tasa de aprendizaje (lr) alto porque el dataset es pequeño (y el modelo)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc_c2wRBo0EC"
   },
   "source": [
    "A continuación entrenamos el modelo en nuestro conjunto de datos. Se parece a las funciones de entrenamiento típicas que hemos visto hasta ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0MbiV_rpo0EC"
   },
   "outputs": [],
   "source": [
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    #Creamos el módulo PyTorch Lightning para entrenamiento con retorno\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=500,\n",
    "                         enable_progress_bar=False)\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    #Comprobamos si existe ya el modelo preentrenado, y lo cargamos si es así\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Encontrado un modelo preentrenado, cargando...\")\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(c_in=mutag_dataset.num_node_features,\n",
    "                              c_out=1 if mutag_dataset.num_classes==2 else mutag_dataset.num_classes,\n",
    "                              **model_kwargs)\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    #Obtenemos el mejor modelo de test en los conjuntos de validación y de test\n",
    "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC-uRvN_o0EC"
   },
   "source": [
    "Por último, vamos a realizar el entrenamiento y las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihPD2hgWo0EC",
    "outputId": "712d3e60-7e7a-4201-83d2-87cdf5d4c594",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
      "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ../saved_models/tarea1/GraphLevelGraphConv/lightning_logs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | model       | GraphGNNModel     | 266 K \n",
      "1 | loss_module | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------------\n",
      "266 K     Trainable params\n",
      "0         Non-trainable params\n",
      "266 K     Total params\n",
      "1.067     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "model, result = train_graph_classifier(model_name=\"GraphConv\",\n",
    "                                       c_hidden=256,\n",
    "                                       layer_name=\"GraphConv\",\n",
    "                                       num_layers=3,\n",
    "                                       dp_rate_linear=0.5,\n",
    "                                       dp_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Mp_Udkbo0ED",
    "outputId": "bf0fc819-d843-44d9-f531-877a2add1b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance: 93.28%\n",
      "Test performance:  92.11%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
    "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH_b0jIqo0ED"
   },
   "source": [
    "**Pregunta 5:** ¿Hay overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEtWZauwm7Yi"
   },
   "source": [
    "**Ejercicio 6:** \"Trastear\" con diferentes capas GNN, hiperparámetros, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ClaS9xl1wsg"
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "Hemos visto la aplicación de las redes neuronales a las estructuras de grafos. Hemos visto cómo se puede representar un grafo (matriz de adyacencia o lista de aristas), y hemos discutido la implementación de capas de grafos comunes: GCN y GAT. Las implementaciones mostraron el lado práctico de las capas, que a menudo es más sencillo que la teoría. Por último, experimentamos con distintas tareas a nivel de nodos, aristas y grafos. En general, hemos visto que incluir información sobre grafos en las predicciones puede ser crucial para lograr un alto rendimiento. Hay muchas aplicaciones que se benefician de las GNN, y es probable que la importancia de estas redes aumente en los próximos años."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzpy-9LFnTWd"
   },
   "source": [
    "**Ejercicio 7:** Tal y como entiendes inicialmente el concepto de razonamiento computacional bajo incertidumbre, por algún ejemplo de problema real con las GNNs que necesitaría tratar la incertidumbre."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "XPSaU-uRo0Dm",
    "Ml1-ga1Oo0D1",
    "y1sBtVhZo0D-",
    "QSInVvkfo0D-",
    "3ClaS9xl1wsg"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
